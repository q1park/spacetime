{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacetime.spacetime import SpaceTime\n",
    "from spacetime.simulate import Simulator\n",
    "from spacetime.metrics import count_accuracy, adjacency_error\n",
    "from spacetime.models import MLPEncoder, MLPDecoder\n",
    "from spacetime.training import train, acyclicity\n",
    "from spacetime.utils import arguments, spacetime_mutilator, graph_clipper\n",
    "from spacetime.sampler import NodeData, GraphSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating a random 3-degree erdos-renyi dag with range (0.5, 2.0) (seed 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -0.89683342,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , -0.5281847 , -1.41814358],\n",
       "       [ 0.        ,  0.        ,  0.        , -1.50015007,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.15554793],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = arguments(node_dict = {0:[0,1], 1:[2], 2:[3], 3:[4]})\n",
    "args.data_sample_size=10000\n",
    "args.noise_scale = 0.6\n",
    "args.graph_type='erdos-renyi'\n",
    "args.graph_sem_type='linear-gauss'\n",
    "args.graph_linear_type='linear'\n",
    "args.graph_degree = 3\n",
    "\n",
    "# training hyperparameters\n",
    "args.graph_threshold=0.3  # 0.3 is good 0.2 is error prune\n",
    "args.tau_A=1e-8\n",
    "args.ordered_graph=True\n",
    "args.use_A_connect_loss=False\n",
    "args.use_A_positiver_loss=False\n",
    "\n",
    "args.seed=42\n",
    "args.epochs=5\n",
    "args.batch_size=100 # note: should be divisible by sample size otherwise throw an error\n",
    "args.encoder_hidden=64\n",
    "args.decoder_hidden=64\n",
    "\n",
    "g = SpaceTime.from_spacelike([0,1,3,2,4], simulate=True,\n",
    "                             degree=args.graph_degree, graph_type=args.graph_type, \n",
    "                             w_range=(0.5,2.0), force_positive=False, seed=0)\n",
    "g.show_adj()\n",
    "g.draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_plot_dict = {0:(4,3), 1:(4,1), 2:(3,2), 3:(2,0)}\n",
    "mutilate = 3\n",
    "observe = 4\n",
    "latent = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating 10000 samples from a linear-gauss sem with linear causal effects\n"
     ]
    }
   ],
   "source": [
    "g.data = Simulator.sem(graph=g.graph, n=args.data_sample_size, x_dims=args.x_dims, \n",
    "                       sem_type=args.graph_sem_type, linear_type=args.graph_linear_type, \n",
    "                       noise_scale=args.noise_scale, seed=args.seed)\n",
    "train_loader, test_loader = g.torch_loader(g.data, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_trials = 1\n",
    "\n",
    "# shd_trials = list()\n",
    "# err_trials = list()\n",
    "# kl_trials = list()\n",
    "# nll_trials = list()\n",
    "# elbo_trials = list()\n",
    "# graph_trials = list()\n",
    "\n",
    "# for trial in range(n_trials):\n",
    "#     shd_train, err_train = list(), list()\n",
    "#     kl_train, nll_train, elbo_train = list(), list(), list()\n",
    "#     hA_train = list()\n",
    "#     graph_train = list()\n",
    "\n",
    "#     best_epoch = 0\n",
    "#     best_ELBO, best_ELBO_graph = np.inf, None\n",
    "#     best_KL, best_KL_graph = np.inf, None\n",
    "#     best_NLL, best_NLL_graph = np.inf, None\n",
    "\n",
    "#     # optimizer step on hyparameters\n",
    "#     c_A = args.c_A\n",
    "#     lambda_A = args.lambda_A\n",
    "    \n",
    "#     h_A_old, h_A_new = np.inf, torch.tensor(1.)\n",
    "#     h_tol = args.h_tol\n",
    "#     k_max_iter = int(args.k_max_iter)\n",
    "\n",
    "#     #===================================\n",
    "#     # load modules\n",
    "#     #===================================\n",
    "#     # add adjacency matrix A\n",
    "#     num_nodes = args.data_variable_size\n",
    "#     adj_A = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "#     encoder = MLPEncoder(args.x_dims, args.encoder_hidden, int(args.z_dims), adj_A).double()\n",
    "#     decoder = MLPDecoder(args.z_dims, args.x_dims, n_hid=args.decoder_hidden).double()\n",
    "\n",
    "#     #===================================\n",
    "#     # set up training parameters\n",
    "#     #===================================\n",
    "#     optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=args.lr)\n",
    "#     scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_decay, gamma=args.gamma)\n",
    "\n",
    "#     #===================================\n",
    "#     # train model\n",
    "#     #===================================\n",
    "#     t_total = time.time()\n",
    "#     for step_k in range(k_max_iter):\n",
    "#         while c_A < 1e+20:\n",
    "#             for epoch in range(args.epochs):\n",
    "#                 ELBO, KL, NLL, origin_A = train(lambda_A, c_A, optimizer, scheduler, \n",
    "#                                                 encoder, decoder, train_loader, args)\n",
    "#                 graph_clone, graph_full, graph = graph_clipper(origin_A, args.graph_threshold)\n",
    "                \n",
    "#                 if ELBO < best_ELBO:\n",
    "#                     best_epoch = epoch\n",
    "#                     best_ELBO = ELBO\n",
    "#                     best_ELBO_graph = graph\n",
    "#                     best_ELBO_graph_full = graph_full\n",
    "\n",
    "#                 if KL < best_KL:\n",
    "#                     best_KL = KL\n",
    "#                     best_KL_graph = graph\n",
    "\n",
    "#                 if NLL < best_NLL:\n",
    "#                     best_NLL = NLL\n",
    "#                     best_NLL_graph = graph\n",
    "                \n",
    "#                 hA = acyclicity(graph_clone, args)\n",
    "#                 fdr, tpr, fpr, shd, nnz = count_accuracy(g.show_adj(), best_ELBO_graph)\n",
    "#                 err = adjacency_error(g.show_adj(), best_ELBO_graph_full)\n",
    "\n",
    "#                 shd_train.append(shd)\n",
    "#                 err_train.append(err)\n",
    "#                 kl_train.append(best_KL)\n",
    "#                 nll_train.append(best_NLL)\n",
    "#                 elbo_train.append(best_ELBO)\n",
    "#                 hA_train.append(hA)\n",
    "                \n",
    "#             graph_train.append(best_ELBO_graph_full)\n",
    "#             print(\"Optimization Finished!\")\n",
    "#             print(\"Best Epoch: {:04d}\\t\".format(best_epoch),\n",
    "#                   \"ELBO: {:.10f}\".format(best_ELBO),\n",
    "#                   \"KL: {:.10f}\".format(best_KL),\n",
    "#                   \"NLL: {:.10f}\".format(best_NLL))\n",
    "            \n",
    "#             if ELBO > 2 * best_ELBO:\n",
    "#                 break\n",
    "\n",
    "#             # update parameters\n",
    "#             h_A_new = acyclicity(graph_clone, args)\n",
    "#             if h_A_new.item() > 0.25 * h_A_old:\n",
    "#                 c_A*=10\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "#         h_A_old = h_A_new.item()\n",
    "#         lambda_A += c_A * h_A_new.item()\n",
    "\n",
    "#         if h_A_new.item() <= h_tol:\n",
    "#             break\n",
    "\n",
    "#     print(\"\\nTrial %s finished in %s seconds\"%(trial, time.time() - t_total))\n",
    "#     print('Best ELBO Stats: shd %s err %s\\n'%(shd, err))\n",
    "# #     print(best_ELBO_graph)\n",
    "\n",
    "# #     print('Ground truth graph')\n",
    "# #     print(g.show_adj(around=3))\n",
    "    \n",
    "#     shd_trials.append(shd_train)\n",
    "#     err_trials.append(err_train)\n",
    "#     kl_trials.append(kl_train)\n",
    "#     nll_trials.append(nll_train)\n",
    "#     elbo_trials.append(elbo_train)\n",
    "#     graph_trials.append(best_ELBO_graph)\n",
    "# pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 5, figsize = (15,3))\n",
    "\n",
    "# for trial in range(n_trials):\n",
    "#     axs[0].plot(range(len(err_trials[trial])), err_trials[trial])\n",
    "#     axs[1].plot(range(len(shd_trials[trial])), shd_trials[trial])\n",
    "#     axs[2].plot(range(len(kl_trials[trial])), np.log10(kl_trials[trial]))\n",
    "#     axs[3].plot(range(len(nll_trials[trial])), np.log10(nll_trials[trial]))\n",
    "#     axs[4].plot(range(len(elbo_trials[trial])), np.log10(elbo_trials[trial]))\n",
    "# pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def draw(data, x, y, ax):\n",
    "    sns.heatmap(data, xticklabels=x, square=True, yticklabels=y, \n",
    "                vmin=0.0, vmax=1.0, cbar=False, ax=ax)\n",
    "    \n",
    "def node_hot(x, pad=(0,0)):\n",
    "    expanded = torch.eye(x.shape[1])*x.unsqueeze(1)\n",
    "    return F.pad(expanded, pad=(*pad, 0, 0), mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_embed, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_embed)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_embed, 2) *\n",
    "                             -(math.log(10000.0) / d_embed))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_in, d_hidden, d_out, dropout=0.1):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hidden)\n",
    "        self.w_2 = nn.Linear(d_hidden, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(EmbedBlock, self).__init__()\n",
    "        self.proj = nn.Linear(d_in, d_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, d_embed, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(d_embed, dropout), 2)\n",
    "        self.d_embed = d_embed\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, d_embed, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = SublayerConnection(d_embed, dropout)\n",
    "        self.norm = LayerNorm(d_embed)\n",
    " \n",
    "    def forward(self, x, memory):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.norm(self.self_attn(x, x, x))\n",
    "        x = self.norm(self.src_attn(x, m, m))\n",
    "        return self.sublayer(x, self.feed_forward)\n",
    "    \n",
    "# class DecoderLayer(nn.Module):\n",
    "#     \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "#     def __init__(self, d_embed, self_attn, src_attn, feed_forward, dropout):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "#         self.d_embed = d_embed\n",
    "#         self.self_attn = self_attn\n",
    "#         self.src_attn = src_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         self.sublayer = clones(SublayerConnection(d_embed, dropout), 3)\n",
    " \n",
    "#     def forward(self, x, memory):\n",
    "#         \"Follow Figure 1 (right) for connections.\"\n",
    "#         m = memory\n",
    "#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "#         x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m))\n",
    "#         return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def clone_encoder(mask, N, **kwargs):\n",
    "    d_embed, d_hidden = kwargs['d_embed'], kwargs['d_hidden']\n",
    "    self_attns = [CausalAttention(n_heads, d_embed, mask=mask) for _ in range(N)]\n",
    "    feed_forwards = [LinearBlock(d_embed, d_hidden, d_embed, dropout=0.1) for _ in range(N)]\n",
    "    modules = [EncoderLayer(d_embed, self_attn, feed_forward, dropout) \n",
    "               for self_attn, feed_forward in zip(self_attns, feed_forwards)]\n",
    "\n",
    "    return nn.ModuleList(modules)\n",
    "\n",
    "def clone_decoder(mask, N, **kwargs):\n",
    "    d_embed, d_hidden = kwargs['d_embed'], kwargs['d_hidden']\n",
    "    self_attns = [CausalAttention(n_heads, d_embed, mask=mask, mask_self=True) for _ in range(N)]\n",
    "    src_attns = [CausalAttention(n_heads, d_embed, mask=mask, mask_self=False) for _ in range(N)]\n",
    "    feed_forwards = [LinearBlock(d_embed, d_hidden, d_embed, dropout=0.1) for _ in range(N)]\n",
    "    \n",
    "    modules = [DecoderLayer(d_embed, self_attn, src_attn, feed_forward, dropout) \n",
    "               for self_attn, src_attn, feed_forward in zip(self_attns, src_attns, feed_forwards)]\n",
    "    return nn.ModuleList(modules)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, mask, n_nodes, d_embed, d_hidden, n_layers, n_heads, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mask = mask\n",
    "        self.layers = clone_encoder(self.mask, n_layers, d_embed=d_embed, d_hidden=d_hidden, dropout=dropout)\n",
    "        self.norm = LayerNorm(self.layers[0].d_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Pass the input through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, mask, n_nodes, d_embed, d_hidden, n_layers, n_heads, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mask = mask\n",
    "        self.layers = clone_decoder(self.mask, n_layers, d_embed=d_embed, d_hidden=d_hidden, dropout=dropout)\n",
    "        self.norm = LayerNorm(self.layers[0].d_embed)\n",
    "        \n",
    "    def forward(self, x, memory):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testt = torch.Tensor(g.show_adj())\n",
    "# testt[testt!=0]=1.0\n",
    "# testt=testt+1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes, d_embed, d_hidden, n_layers, n_heads, dropout):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.e_mask = torch.eye(n_nodes).unsqueeze(0)\n",
    "#         self.e_mask = torch.tril(torch.ones((1, n_nodes, n_nodes)), diagonal=-1)\n",
    "#         self.d_mask = nn.Parameter(torch.tril(torch.ones((1, n_nodes, n_nodes)), diagonal=-1), requires_grad=True)\n",
    "#         self.d_mask = nn.Parameter(torch.ones((1, n_nodes, n_nodes)), requires_grad=True)\n",
    "        self.d_mask = nn.Parameter(torch.ones((1, n_nodes, n_nodes)), requires_grad=True)\n",
    "#         self.d_mask = testt.transpose(0,1)\n",
    "        self.encoder = Encoder(self.e_mask, n_nodes, d_embed, d_hidden, n_layers, n_heads, dropout)\n",
    "        self.decoder = Decoder(self.d_mask, n_nodes, d_embed, d_hidden, n_layers, n_heads, dropout)\n",
    "        self.embedder = EmbedBlock(n_nodes, d_embed)\n",
    "#         self.embedder = nn.Sequential(EmbedBlock(n_nodes, d_embed), PositionalEncoding(d_embed, dropout))\n",
    "        self.generator = EmbedBlock(d_embed, n_nodes)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # This was important from their code. \n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        src = self.embedder(src)\n",
    "        src = self.encoder(src)\n",
    "        return src\n",
    "    \n",
    "    def decode(self, tgt, memory):\n",
    "        tgt = self.embedder(tgt)\n",
    "        tgt = self.decoder(tgt, memory)\n",
    "        return tgt\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        memory = self.encode(src)\n",
    "        tgt = self.decode(tgt, memory)\n",
    "        tgt = self.generator(tgt)\n",
    "        \n",
    "        return tgt, memory, self.d_mask # F.softmax(-torch.exp(-(1e-10+torch.abs(self.d_mask)).log()), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, mask, mask_self=True, dropout=0.1):\n",
    "        \"Take in model size and number of n_headseads.\"\n",
    "        super(CausalAttention, self).__init__()\n",
    "        assert d_embed % n_heads == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_embed // n_heads\n",
    "        self.h = n_heads\n",
    "        self.linears = clones(nn.Linear(d_embed, d_embed), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.eps = 1e-10\n",
    "        self.mask = mask\n",
    "        self.extra_linear = nn.Linear(d_embed, d_embed)\n",
    "        self.norm = LayerNorm(d_embed)\n",
    "        \n",
    "        if mask_self:\n",
    "            self.mask_add = torch.zeros((mask.shape[-1], mask.shape[-1])).unsqueeze(0)\n",
    "        else:\n",
    "            self.mask_add = torch.eye(mask.shape[-1]).unsqueeze(0)\n",
    "            \n",
    "    def add_mask(self):\n",
    "        return (self.eps+self.mask_add+torch.abs(self.mask)).unsqueeze(1)\n",
    "            \n",
    "    def get_mask(self):\n",
    "        return -torch.exp(-self.add_mask().log())\n",
    "    \n",
    "    def attention(self, query, key, value, dropout=None):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "#         scores += torch.abs(((self.mask_add+self.mask).unsqueeze(1)+self.eps)).log()\n",
    "#         scores += self.penalty()\n",
    "#         p_attn = F.softmax(scores, dim = -1)\n",
    "        p_attn = F.softmax(scores, dim = -1)*(self.mask_add+torch.abs(self.mask)).unsqueeze(1)\n",
    "    \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    \n",
    "    def bmm(self, adj, x):\n",
    "        return torch.einsum('ij,ajc->aic', adj, x)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        \"Implements Figure 2\"\n",
    "        nbatches = query.size(0)\n",
    "        # batchwise linear projections from d_embed => n_heads x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        x, self.attn = self.attention(query, key, value, dropout=self.dropout)\n",
    "        \n",
    "        # view contatentation and apply final linear layer \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h*self.d_k)\n",
    "#         print((self.mask_add+torch.abs(self.mask)).shape, self.linears[-1](x).shape)\n",
    "        return self.linears[-1](x)\n",
    "#         return self.extra_linear(F.relu(self.linears[-1](x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecBatch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, tgt=None, pad=0):\n",
    "        self.src = node_hot(src)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :]#node_hot(tgt[:, :], pad=(0,0))\n",
    "            self.tgt_y = tgt[:, :]#node_hot(tgt[:, :], pad=(0,0))\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "            \n",
    "def data_graph_gen(spacetime, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.squeeze(torch.from_numpy(g.data[i*batch:(i+1)*batch]))\n",
    "        src = Variable(data, requires_grad=False).float()\n",
    "        tgt = Variable(data, requires_grad=False).float()\n",
    "        yield VecBatch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacetime.training_att import LagrangeLoss, ActionOpt\n",
    "from spacetime.training_att import h_A, h_A_timed\n",
    "from spacetime.utils_att import Parameters\n",
    "# from spacetime.training_att import train, truth_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss):\n",
    "    t = time.time()\n",
    "    model.train()    \n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        preds, z_train, origin_A = model(batch.src.float(), batch.tgt.float())\n",
    "        if torch.sum(preds != preds):\n",
    "            raise ValueError('nan error\\n')\n",
    "        if torch.sum(origin_A != origin_A):\n",
    "            raise ValueError('nan error\\n')\n",
    "\n",
    "        loss(origin_A.squeeze().transpose(0,1), preds, batch.src.float(), z_train)\n",
    "    loss.end_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "n_nodes=5\n",
    "d_embed = 10\n",
    "d_hidden=5*d_embed\n",
    "\n",
    "n_layers=2\n",
    "n_heads = 5\n",
    "dropout=0.1\n",
    "\n",
    "trn_params = Parameters(batch_size=100, epochs=5)\n",
    "opt_params = Parameters(constraint=lambda x: h_A(x, n_nodes),\n",
    "                        lr=0.001, l=1e-2, c=1, h=np.inf, tau=1e-10, \n",
    "                        max_iters=50, h_tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [300 x 5], m2: [1 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b721e4c62210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0moptimizerL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0moptimizerL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_graph_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#             shd, err = truth_evaluation(g.adj, optimizerL, mod_params.graph_threshold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#             shd_train.append(shd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-b1f9c05a6182>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan error\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-17260ba71c81>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m\"Take in and process masked src and target sequences.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-17260ba71c81>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-49870581e0a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [300 x 5], m2: [1 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "n_trials = 1\n",
    "shd_trials, err_trials = list(), list()\n",
    "graph_trials = list()\n",
    "\n",
    "loss_log = {k:[] for k in ('elbo', 'kld', 'nll')}\n",
    "param_log = {k:[] for k in ('lr', 'l', 'c', 'h')}\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    shd_train, err_train = list(), list()\n",
    "    graph_train = list()\n",
    "    \n",
    "    #===================================\n",
    "    # load modules\n",
    "    #===================================\n",
    "\n",
    "    autoencoder = EncoderDecoder(n_nodes, d_embed, d_hidden, n_layers, n_heads, dropout)\n",
    "    optimizerL = ActionOpt(autoencoder, opt_params, h_factor=0.25*(2), c_factor=10.0/(3), warmups=500)\n",
    "    lossL = LagrangeLoss(opt=optimizerL)\n",
    "    \n",
    "    #===================================\n",
    "    # train model\n",
    "    #===================================\n",
    "    \n",
    "    t_total = time.time()\n",
    "    while optimizerL._iter < optimizerL.max_iter:\n",
    "        for epoch in range(trn_params.epochs):\n",
    "            train(autoencoder, data_graph_gen(g, 60, 60), lossL)\n",
    "#             shd, err = truth_evaluation(g.adj, optimizerL, mod_params.graph_threshold)\n",
    "#             shd_train.append(shd)\n",
    "#             err_train.append(err)\n",
    "#             graph_train.append(optimizerL.adj.data.clone())\n",
    "        \n",
    "        print(\"Iteration: %s, Best Epoch: %s/%s\"%(optimizerL._iter, optimizerL.best_epoch, trn_params.epochs))\n",
    "        print(\"   ELBO: {:.7f}, KL: 10^{:.3f}, NLL: 10^{:.3f} || h: 10^{:.3f}, c: 10^{:.3f}, l: {:.7f}, lr: {:.7f}\".format(\n",
    "            optimizerL.min_elbo, np.log10(optimizerL.min_kld), np.log10(optimizerL.min_nll),\n",
    "            np.log10(optimizerL.h), np.log10(optimizerL.c), optimizerL.l, optimizerL.log['lr'][-1])\n",
    "        )\n",
    "\n",
    "        # update parameters\n",
    "        optimizerL.iterate()\n",
    "        if optimizerL._iter > 10: #or optimizerL.h <= optimizerL.h_tol:\n",
    "            break\n",
    "    \n",
    "    print(\"\\nTrial %s finished in %s seconds\"%(trial, time.time() - t_total))\n",
    "#     print('Best ELBO Stats: shd %s err %s\\n'%(shd, err))\n",
    "\n",
    "#     shd_trials.append(shd_train)\n",
    "#     err_trials.append(err_train)\n",
    "#     graph_trials.append(optimizerL.show_adj(mod_params.graph_threshold))\n",
    "    \n",
    "    for k,v in loss_log.items():\n",
    "        v += [optimizerL.log[k]]\n",
    "        \n",
    "    for k,v in param_log.items():\n",
    "        v += [optimizerL.log[k]]\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1193, -0.4492,  0.4254,  0.0621,  0.0237,  0.2819,  0.0104, -0.0384,\n",
       "         -0.2486,  0.2828],\n",
       "        [-0.3062, -0.1032,  0.2413,  0.3355, -0.4903,  0.3087,  0.0521,  0.3256,\n",
       "         -0.1620,  0.1895],\n",
       "        [ 0.1029,  0.2870,  0.4716,  0.2158, -0.2303,  0.1722, -0.0608,  0.0098,\n",
       "          0.1577,  0.1352],\n",
       "        [-0.0458, -0.3567,  0.0708,  0.2658, -0.2714,  0.2759, -0.1890,  0.1003,\n",
       "          0.4124, -0.2212],\n",
       "        [ 0.2419,  0.1536,  0.5711,  0.3045, -0.3411,  0.2656, -0.1090, -0.5575,\n",
       "         -0.3022,  0.2990],\n",
       "        [ 0.1037,  0.4232,  0.2404, -0.1859, -0.5152,  0.5648, -0.1156,  0.3097,\n",
       "         -0.0942,  0.5054],\n",
       "        [ 0.7625,  0.6232, -0.0817, -0.1277,  0.5739, -0.5966, -0.0820, -0.4224,\n",
       "          0.0364, -0.3697],\n",
       "        [-0.0877,  0.2231, -0.5447, -0.0317, -0.2013, -0.2952,  0.4793, -0.3667,\n",
       "          0.3009, -0.2715],\n",
       "        [-0.5126, -0.4344, -0.3224,  0.2834,  0.2986,  0.0407, -0.1337,  0.2643,\n",
       "         -0.2885, -0.3725],\n",
       "        [-0.3459, -0.1690,  0.0503, -0.2753, -0.0869, -0.3276, -0.5226,  0.0368,\n",
       "         -0.3778,  0.1948]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.decoder.layers[1].self_attn.linears[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize = (12,3))\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    axs[0].plot(range(len(loss_log['nll'][trial])), np.log10(loss_log['nll'][trial]))\n",
    "    axs[1].plot(range(len(param_log['c'][trial])), np.log10(param_log['c'][trial]))\n",
    "    axs[2].plot(range(len(param_log['lr'][trial])), np.log10(param_log['lr'][trial]))\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(adj, threshold):\n",
    "    return nn.Threshold(threshold, 0.0)(adj)-nn.Threshold(threshold, 0.0)(-adj)\n",
    "\n",
    "def time_scores(adj):\n",
    "    n_nodes = adj.shape[0]\n",
    "    I = torch.eye(n_nodes).double()\n",
    "    expA = torch.matrix_power(I+(1/n_nodes)*torch.tanh(adj)**2, n_nodes)\n",
    "    scores = torch.div(1.0, torch.sum(expA, dim=1))-torch.div(1.0, torch.sum(expA, dim=0))\n",
    "    return scores\n",
    "\n",
    "def time_sort(adj):\n",
    "    return torch.argsort(time_scores(adj.squeeze()))\n",
    "\n",
    "testdata = iter(data_graph_gen(g, 30, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(testdata)\n",
    "preds, z_train, origin_A = autoencoder(batch.src.float(), batch.tgt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2987,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.2391,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.3398,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000, -2.3008,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000, -2.9215]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4071,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.2980,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.5741,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000, -2.2069,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000, -3.2560]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold(preds[:1],0.2)\n",
    "threshold(batch.src.float()[:1], 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9022e-04, -7.9821e-03, -3.3828e-03,  1.0188e-02, -7.9877e-03],\n",
       "         [-2.0454e-02,  4.0573e-03, -1.2769e-02, -2.7223e-02, -5.3379e-02],\n",
       "         [-7.4511e-05,  3.1597e-05, -7.6642e-06, -5.8616e-02,  3.1565e-04],\n",
       "         [ 1.0150e-02, -1.2034e-02,  2.6101e-03, -1.1727e-01,  2.3391e-02],\n",
       "         [-6.5065e-03, -9.3396e-03,  1.8781e-02, -7.8410e-02, -1.3961e-01]],\n",
       "\n",
       "        [[-1.3068e-04,  2.2219e-03, -1.9704e-03,  5.0353e-03, -1.3130e-03],\n",
       "         [-9.2098e-03, -1.1294e-03, -7.4378e-03, -1.3455e-02, -8.7741e-03],\n",
       "         [-3.3549e-05, -8.7953e-06, -4.4643e-06, -2.8971e-02,  5.1885e-05],\n",
       "         [ 4.5700e-03,  3.3497e-03,  1.5203e-03, -5.7960e-02,  3.8448e-03],\n",
       "         [-2.9296e-03,  2.5998e-03,  1.0939e-02, -3.8754e-02, -2.2949e-02]]],\n",
       "       dtype=torch.float64, grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9870e-01,  1.3985e-01, -5.0413e-02,  2.7639e-02, -1.1741e-01],\n",
       "         [-6.7768e-02,  2.3912e-01, -6.4426e-02, -1.3186e-01, -8.4290e-02],\n",
       "         [ 2.2298e-02, -1.0419e-01,  3.3979e-01, -2.7972e-02,  7.8569e-03],\n",
       "         [-4.0249e-02, -7.7361e-02, -6.6776e-02, -2.3008e+00, -2.4949e-02],\n",
       "         [ 1.7619e-01, -8.7317e-02, -6.9571e-02,  1.2637e-02, -2.9215e+00]],\n",
       "\n",
       "        [[-1.0076e-01, -3.9031e-02,  7.6385e-02, -8.7580e-04,  3.2760e-02],\n",
       "         [-6.5328e-02, -8.8365e-03, -5.2924e-02,  1.3918e-02, -9.9846e-02],\n",
       "         [ 2.3413e-02, -3.4533e-02,  2.8645e-01, -3.6898e-03, -1.1689e-01],\n",
       "         [ 8.8663e-02, -1.0485e-02,  1.1543e-01, -1.1473e+00, -5.5003e-02],\n",
       "         [ 2.0078e-01,  6.0658e-02, -1.2348e-02, -5.8608e-02, -6.4431e-01]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmm(origin_A.double(), batch.src.double())[:2]\n",
    "preds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2984, -0.1478,  0.0470, -0.0175,  0.1094],\n",
       "         [ 0.0473, -0.2351,  0.0517,  0.1046,  0.0309],\n",
       "         [-0.0224,  0.1042, -0.3398, -0.0306, -0.0075],\n",
       "         [ 0.0504,  0.0653,  0.0694,  2.1835,  0.0483],\n",
       "         [-0.1827,  0.0780,  0.0884, -0.0910,  2.7819]],\n",
       "\n",
       "        [[ 0.1006,  0.0413, -0.0784,  0.0059, -0.0341],\n",
       "         [ 0.0561,  0.0077,  0.0455, -0.0274,  0.0911],\n",
       "         [-0.0234,  0.0345, -0.2865, -0.0253,  0.1169],\n",
       "         [-0.0841,  0.0138, -0.1139,  1.0893,  0.0588],\n",
       "         [-0.2037, -0.0581,  0.0233,  0.0199,  0.6214]]], dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bmm(origin_A.double(), batch.src.double())-preds.double())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 7.1291e-04, -2.6783e-02, -5.8926e-03, -4.6161e-03,  2.4532e-03],\n",
       "         [ 5.0244e-02,  1.3614e-02, -2.2243e-02,  1.2335e-02,  1.6394e-02],\n",
       "         [ 1.8303e-04,  1.0602e-04, -1.3351e-05,  2.6560e-02, -9.6944e-05],\n",
       "         [-2.4932e-02, -4.0378e-02,  4.5466e-03,  5.3135e-02, -7.1838e-03],\n",
       "         [ 1.5983e-02, -3.1338e-02,  3.2715e-02,  3.5529e-02,  4.2879e-02]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0268,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0502,  0.0136, -0.0222,  0.0123,  0.0164],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0266,  0.0000],\n",
       "        [-0.0249, -0.0404,  0.0000,  0.0531,  0.0000],\n",
       "        [ 0.0160, -0.0313,  0.0327,  0.0355,  0.0429]],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold(autoencoder.d_mask, 0.01).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 7.1291e-04, -2.6783e-02, -5.8926e-03, -4.6161e-03,  2.4532e-03],\n",
       "         [ 5.0244e-02,  1.3614e-02, -2.2243e-02,  1.2335e-02,  1.6394e-02],\n",
       "         [ 1.8303e-04,  1.0602e-04, -1.3351e-05,  2.6560e-02, -9.6944e-05],\n",
       "         [-2.4932e-02, -4.0378e-02,  4.5466e-03,  5.3135e-02, -7.1838e-03],\n",
       "         [ 1.5983e-02, -3.1338e-02,  3.2715e-02,  3.5529e-02,  4.2879e-02]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 7.1291e-04, -2.6783e-02, -5.8926e-03, -4.6161e-03,  2.4532e-03],\n",
       "         [ 5.0244e-02,  1.3614e-02, -2.2243e-02,  1.2335e-02,  1.6394e-02],\n",
       "         [ 1.8303e-04,  1.0602e-04, -1.3351e-05,  2.6560e-02, -9.6944e-05],\n",
       "         [-2.4932e-02, -4.0378e-02,  4.5466e-03,  5.3135e-02, -7.1838e-03],\n",
       "         [ 1.5983e-02, -3.1338e-02,  3.2715e-02,  3.5529e-02,  4.2879e-02]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0268,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0502,  0.0136, -0.0222,  0.0123,  0.0164],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0266,  0.0000],\n",
       "         [-0.0249, -0.0404,  0.0000,  0.0531,  0.0000],\n",
       "         [ 0.0160, -0.0313,  0.0327,  0.0355,  0.0429]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0049, dtype=torch.float64, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.d_mask\n",
    "autoencoder.e_mask\n",
    "threshold(autoencoder.d_mask, 0.01)\n",
    "\n",
    "optimizerL.constraint(autoencoder.d_mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm(adj, data):\n",
    "    return torch.einsum('ij,ajc->aic', adj.squeeze(), data.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9107, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.sum(torch.abs(bmm(origin_A.double(), batch.src.double())-preds.double()), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.9022e-04, 7.9821e-03, 3.3828e-03, 1.0188e-02, 7.9877e-03],\n",
       "         [2.0454e-02, 4.0573e-03, 1.2769e-02, 2.7223e-02, 5.3379e-02],\n",
       "         [7.4511e-05, 3.1597e-05, 7.6642e-06, 5.8616e-02, 3.1565e-04],\n",
       "         [1.0150e-02, 1.2034e-02, 2.6101e-03, 1.1727e-01, 2.3391e-02],\n",
       "         [6.5065e-03, 9.3396e-03, 1.8781e-02, 7.8410e-02, 1.3961e-01]],\n",
       "\n",
       "        [[1.3068e-04, 2.2219e-03, 1.9704e-03, 5.0353e-03, 1.3130e-03],\n",
       "         [9.2098e-03, 1.1294e-03, 7.4378e-03, 1.3455e-02, 8.7741e-03],\n",
       "         [3.3549e-05, 8.7953e-06, 4.4643e-06, 2.8971e-02, 5.1885e-05],\n",
       "         [4.5700e-03, 3.3497e-03, 1.5203e-03, 5.7960e-02, 3.8448e-03],\n",
       "         [2.9296e-03, 2.5998e-03, 1.0939e-02, 3.8754e-02, 2.2949e-02]]],\n",
       "       dtype=torch.float64, grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9870e-01,  1.3985e-01, -5.0413e-02,  2.7639e-02, -1.1741e-01],\n",
       "         [-6.7768e-02,  2.3912e-01, -6.4426e-02, -1.3186e-01, -8.4290e-02],\n",
       "         [ 2.2298e-02, -1.0419e-01,  3.3979e-01, -2.7972e-02,  7.8569e-03],\n",
       "         [-4.0249e-02, -7.7361e-02, -6.6776e-02, -2.3008e+00, -2.4949e-02],\n",
       "         [ 1.7619e-01, -8.7317e-02, -6.9571e-02,  1.2637e-02, -2.9215e+00]],\n",
       "\n",
       "        [[-1.0076e-01, -3.9031e-02,  7.6385e-02, -8.7580e-04,  3.2760e-02],\n",
       "         [-6.5328e-02, -8.8365e-03, -5.2924e-02,  1.3918e-02, -9.9846e-02],\n",
       "         [ 2.3413e-02, -3.4533e-02,  2.8645e-01, -3.6898e-03, -1.1689e-01],\n",
       "         [ 8.8663e-02, -1.0485e-02,  1.1543e-01, -1.1473e+00, -5.5003e-02],\n",
       "         [ 2.0078e-01,  6.0658e-02, -1.2348e-02, -5.8608e-02, -6.4431e-01]]],\n",
       "       dtype=torch.float64, grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(bmm(origin_A.double(), batch.src.double()))[:2]\n",
    "preds.double()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.89683342,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.5281847 , -1.50015007,  0.        ,  0.        ],\n",
       "       [ 0.        , -1.41814358,  0.        ,  1.15554793,  0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.show_adj().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testt2 = torch.Tensor([[ 2.6456e-06, -9.5818e-08, -8.6945e-01,  2.0426e-02,  5.9849e-03],\n",
    "        [-1.0854e-07,  2.3277e-06,  2.6204e-02, -5.4377e-01, -1.4878e+00],\n",
    "        [-1.7013e-05,  2.5015e-09,  2.7552e-06, -1.5096e+00,  3.8229e-02],\n",
    "        [ 2.6729e-05, -1.0530e-05, -2.9288e-05,  2.4118e-06,  1.2162e+00],\n",
    "        [ 3.2156e-05, -4.2439e-05, -3.4324e-05,  2.3741e-05,  2.5642e-06]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7797e+05, -1.0447e+07, -1.1502e+00, -4.8957e+01, -1.6709e+02],\n",
       "        [-9.2217e+06, -4.2959e+05, -3.8162e+01, -1.8390e+00, -6.7213e-01],\n",
       "        [-5.8779e+04, -3.8439e+08, -3.6294e+05, -6.6243e-01, -2.6158e+01],\n",
       "        [-3.7412e+04, -9.4968e+04, -3.4144e+04, -4.1461e+05, -8.2223e-01],\n",
       "        [-3.1098e+04, -2.3563e+04, -2.9134e+04, -4.2121e+04, -3.8997e+05]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00, 1.7285e-21, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 3.9865e-17, 2.3742e-01, 7.6258e-01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 8.4596e-12],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.exp(-(torch.abs(testt2+1e-10).log()))\n",
    "F.softmax(-torch.exp(-(torch.abs(testt2)+1e-10).log()), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 3.1659e-01, 5.4721e-22, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 2.6693e-17, 1.5897e-01, 5.1062e-01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1560e-01, 4.3617e-12],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3945e-01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(-torch.exp(-(torch.abs(testt2)+1e-10).log()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7797e+05, -1.0426e+07, -1.1502e+00, -4.8957e+01, -1.6709e+02],\n",
       "        [-9.2047e+06, -4.2959e+05, -3.8162e+01, -1.8390e+00, -6.7213e-01],\n",
       "        [-5.8778e+04, -3.8439e+08, -3.6294e+05, -6.6243e-01, -2.6158e+01],\n",
       "        [-3.7412e+04, -9.4966e+04, -3.4144e+04, -4.1461e+05, -8.2223e-01],\n",
       "        [-3.1098e+04, -2.3563e+04, -2.9134e+04, -4.2121e+04, -3.8997e+05]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.exp(-(torch.abs(testt2)+1e-10).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7797e+05, -1.0426e+07, -1.1502e+00, -4.8957e+01, -1.6709e+02],\n",
       "        [-9.2047e+06, -4.2959e+05, -3.8162e+01, -1.8390e+00, -6.7213e-01],\n",
       "        [-5.8778e+04, -3.8439e+08, -3.6294e+05, -6.6243e-01, -2.6158e+01],\n",
       "        [-3.7412e+04, -9.4966e+04, -3.4144e+04, -4.1461e+05, -8.2223e-01],\n",
       "        [-3.1098e+04, -2.3563e+04, -2.9134e+04, -4.2121e+04, -3.8997e+05]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/(torch.abs(testt2)+1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6456e-06, 9.5818e-08, 8.6945e-01, 2.0426e-02, 5.9849e-03],\n",
       "        [1.0854e-07, 2.3277e-06, 2.6204e-02, 5.4377e-01, 1.4878e+00],\n",
       "        [1.7013e-05, 2.5015e-09, 2.7552e-06, 1.5096e+00, 3.8229e-02],\n",
       "        [2.6729e-05, 1.0530e-05, 2.9288e-05, 2.4118e-06, 1.2162e+00],\n",
       "        [3.2156e-05, 4.2439e-05, 3.4324e-05, 2.3741e-05, 2.5642e-06]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(testt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, -0.8694,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000, -0.5438, -1.4878],\n",
       "        [ 0.0000,  0.0000,  0.0000, -1.5096,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.2162],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7797e+05, -1.0426e+07, -1.1502e+00, -4.8957e+01, -1.6709e+02],\n",
       "        [-9.2047e+06, -4.2959e+05, -3.8162e+01, -1.8390e+00, -6.7213e-01],\n",
       "        [-5.8778e+04, -3.8439e+08, -3.6294e+05, -6.6243e-01, -2.6158e+01],\n",
       "        [-3.7412e+04, -9.4966e+04, -3.4144e+04, -4.1461e+05, -8.2223e-01],\n",
       "        [-3.1098e+04, -2.3563e+04, -2.9134e+04, -4.2121e+04, -3.8997e+05]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2374, 0.7626],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.3166, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1590, 0.5106],\n",
       "        [0.0000, 0.0000, 0.0000, 0.5156, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4394],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold(testt2, 0.1)\n",
    "threshold(-torch.exp(-(torch.abs(testt2)+1e-10).log()), 0.1)\n",
    "threshold(F.softmax(-torch.exp(-(torch.abs(testt2)+1e-10).log()), dim = -1), 0.1)\n",
    "threshold(torch.exp(-torch.exp(-(torch.abs(testt2)+1e-10).log())), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -torch.exp(-(torch.eye(autoencoder.d_mask.shape[1])+1e-10).log())\n",
    "# -torch.exp(-5*(torch.abs(autoencoder.d_mask)+1e-4).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-cc2537df8ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn_heads\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn_heads\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask_norm' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10, 5))\n",
    "draw(torch.abs(autoencoder.d_mask.detach().squeeze()), list(range(5)), list(range(5)) if n_heads ==0 else [], ax=axs[0])\n",
    "draw(torch.abs(mask_norm.detach().squeeze()), list(range(5)), list(range(5)) if n_heads ==0 else [], ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_scores(mask_norm.squeeze().transpose(0,1))\n",
    "torch.argsort(time_scores(mask_norm.squeeze().transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.decoder.layers[1].self_attn.attn[0, 0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = list(range(5))\n",
    "tgt_sent = list(range(5))\n",
    "\n",
    "autoencoder.decoder.layers[0].self_attn.attn[0, 0].data\n",
    "\n",
    "for layer in range(0,2):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 10))\n",
    "    print(\"Decoder Self Layer\", layer+1)\n",
    "    for h in range(1):\n",
    "        draw(autoencoder.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(tgt_sent)], \n",
    "            tgt_sent[:-1], tgt_sent[:-1] if h ==0 else [], ax=axs[h])\n",
    "    plt.show()\n",
    "    print(\"Decoder Src Layer\", layer+1)\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 10))\n",
    "    for h in range(1):\n",
    "        draw(autoencoder.decoder.layers[layer].src_attn.attn[0, h].data[:len(tgt_sent), :len(sent)], \n",
    "            sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
